---
title: "Lab 9: Regression"
author: "Dr. Jiacheng Cai"
date: "2023-07-16"
output: 
  html_document: 
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE,message=FALSE)

```

Student: JJ McCauley

# Introduction

We will be working with data that are sets of ordered pairs of numbers. The ordered pairs are labeled $(x_i,y_i )$ for $i = 1$ to $n$. The $y$'s are the called the response (dependent) variable and the $x$'s are called the control (independent) variable.

**Simple Linear Regression** is widely used to approach the possible relationship between $x$ and $y$, as long as the following assumptions are satisfied.

**Assumptions for Simple Linear Regression Model:**

1.  Each $y_i$ is related to $x_i$ via the equation $y_i=\beta_0+\beta_1x_i+\epsilon_i$;
2.  The $x_i$'s are given: i.e., the $x_i$'s are not random variables;
3.  The $\epsilon_i$'s are **independent normal** random variables with mean 0 and standard deviation $\sigma$;
4.  $\beta_0, \beta_1$, and $\sigma$ are unknown **population parameters** to be estimated from the **sample** data.

When using simple linear regression, there are two major tasks.

I. Determine whether or not it is reasonable to assume our data was generated by the simple linear regression model.

II\. If we answer yes to I., then we need to estimate the numbers $\beta_0, \beta_1$, and $\sigma$ and use our estimates to answer questions, make decisions, etc.

The estimates for $\beta_0, \beta_1$, and $\sigma$ are usually called $\widehat{\beta_0}, \widehat{\beta_1}$, and $\widehat{\sigma}$. And once we have found $\widehat{\beta_0}, \widehat{\beta_1}$, and $\widehat{\sigma}$, then given a value of $x$, we can use $\widehat{\beta_0}, \widehat{\beta_1}$, and $\widehat{\sigma}$ to estimate the mean value of the response variable $y$. This estimate is called $\widehat{y}$ and is calculated as follows:

$$
\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x.
$$

The **residuals** $\widehat{e_i}$ are given by the following equation

$$
\widehat{e_i}=y_i-\widehat{y_i}=y_i-(\widehat{\beta_0}+\widehat{\beta_1}x_i)
$$

and are central in analyzing the simple linear regression model.   Analyzing the residuals can lead to further evidence the assumptions for simple linear regression have or have not been met.  In addition, if the assumptions for the simple linear regression model are satisfied, the residuals carry information about how reliable our estimates might be.

A straightforward way to begin addressing the assumptions for simple linear regression is to scatter plot the ordered pairs and display the line $\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x$ on the same graph. If the scatter plot of the data points seems to scatter non randomly about the line $\widehat{y}=\widehat{\beta_0}+\widehat{\beta_1}x$, then it's likely that one or more of the assumptions for simple linear regression are not satisfied and something else possibly should be done. If the scatter plot looks reasonable then we move forward and do a more thorough analysis of the ordered pairs.

In today's lab, we will first work with simulated data to understand the elementary processes of simple linear regression, then we will work on real word data, and finally we will consider some special examples about regression.

# Part I: Working with Simulated Data

In part I, we first pre-set the relationship between $x$'s and $y$'s (step 0), then simulate sample pairs $(x_i,y_i)$ based on that pre-set relationship (step 1). After the sample pairs are obtained, we "pretend" that we don't know that relationship, and wish to approach/estimate that relationship based on the sample data (step 2).

**Step 0**: We first define that the true relationship between $x$ and $y$ is given by

$$
y_i=2+4x_i+\epsilon_i
$$

**Step 1.1**: Define $x_i$: recall assumption 2, $x_i$ should be given, not random. Here we define $x_i$'s as numbers from 0 through 2 in steps of 0.1. To do that, we use the seq(from, to, by) function. This is the x in part I, we use the object x1 to store the results

```{r}
x1 <- seq(from=0,to=2,by=0.1)
x1
```

**Step 1.2**: Define $\epsilon_i$: recall assumption 3, $\epsilon_i$ should be independent normal random variables with mean 0 and fixed standard deviation (here we set $\sigma=1$). To simulate such random variables, we use function rnorm(number of random variable to generate, mean, standard deviation):

```{r}
eps1 <- rnorm(21,0,1)
```

**Step 1.3**: Simulate $y_i$: This is our first model in part I, we store the results in object y1_1

```{r}
y1_1 <- 2+4*x1+eps1
```

Up to this point, we obtain the 21 sample pairs of $(x_i,y_i)$. Now, let's forget the true relationship and try to approach that relationship. If we assume that the relationship is linear, we use **simple linear regression analysis** to do that.

**Step 2.1**: Obtain the scatter plot of $(x_i,y_i)$

```{r}
plot(y1_1~x1)
title(main="Scatter plot - Model 1 - JJ McCauley")
```

Does the above scatter plot shows an approximately linear pattern? If yes, we can go to the next sub-step

**Step 2.2**: Simple Linear Regression Analysis

```{r}
mod1 <- lm(y1_1~x1) #Creating the Linear Regression Model

#Outputting the results
summary(mod1)
anova(mod1)

plot(mod1)
```

How to interpret the analysis results?

Finally, let's plot the scatter plot with regression line together

```{r}
plot(y1_1~x1)
abline(mod1)
```

What is your observation?

The regression line appears to act as a line of best fit for the data, appearing to fairly closely represent the true data.

## Assignment 1.1

Now mimic the above processes, use the same x1 and eps1, define the second y using y1_2\<-3-2\*x1+eps1, then obtain the scatter plot, check if the relationship between y1_2 and x1 is approximately linear, if yes, run the simple linear regression analysis (store the result in object mod2), interpret the results, and finally plot the scatter plot together with the regression line.

Remark: When you create the plots, make sure you add the title with your name.

R codes:

```{r}
#Defining x1 and epsilon1 
x1 <- seq(from=0,to=2,by=0.1)
eps1 <- rnorm(21,0,1)

#Simulating y1
y1_2 <- 3-2*x1+eps1

#Obtaining the scatter plot of (x1,y1)
plot(y1_2, x1)
title(main="Scatter Plot - Data 2 - JJ McCauley")

#This plot shows a roughly negative linear pattern, so we may use linear regression
model2 <- lm(y1_2~x1)

#Showcasing the results
summary(model2)
anova(model2)
plot(model2)
title(main="Scatter Plot - Model - JJ McCauley")

#Plot the linear regression line on the original plot
plot(y1_2~x1)
title(main="Scatter Plot - Model v Data 2 - JJ McCauley")
abline(model2)
```

Interpretation:

```         
When using the scatterplot to plot the data, it was apparent that there was a rough negative linear relationship with the data, with values of y decreasing and x increased. Therefore, we were able to use a linear regression model. When plotting, it became clear that the line acts as a line of best fit, minimizing the differences between y and x. Additionally, the residuals are around 0 and follow the angle of the QQ plot, leading to the belief that this is a good fit.
```

**End of Assignment 1.1**

Next, we will explore the cases when the assumptions 1 or 3 for the linear regression analysis are not satisfied.

Assumption 1 says that the relationship between $x$ and $y$ are linear. So we define our mod 3 as

$$
y_i=2+4x_i^3+\epsilon_i
$$

where $\epsilon_i$ still satisfies assumption 3, independent normal random variables with mean 0 and standard deviation 1.

## Assignment 1.2

Now mimic the above processes, use the same x1 and eps1, define the second y using y1_3\<-2+4\*x1\^3+eps1, then obtain the scatter plot, check if the relationship between y1_3 and x1 is approximately linear, still run the simple linear regression analysis (store the result in object mod3), but check how reliable the result is, and finally plot the scatter plot together with the regression line.

Remark: When you create the plots, make sure you add the title with your name.

R codes:

```{r}
#Defining x2 and epsilon2 
x2 <- seq(from=0,to=2,by=0.1)
eps2 <- rnorm(21,0,1)

#Simulating y1
y1_3 <- 2+4*x2^3+eps2

#Obtaining the scatter plot of (x1,y1)
plot(y1_3, x1)
title(main="Scatter Plot - Data 3 - JJ McCauley")

#This plot shows a roughly negative linear pattern, so we may use linear regression
model3 <- lm(y1_3~x1)

#Showcasing the results
summary(model3)
anova(model3)
plot(model3)
title(main="Scatter Plot - Model 3 - JJ McCauley")

#Plot the linear regression line on the original plot
plot(y1_3~x1)
title(main="Scatter Plot - Model v Data 3 - JJ McCauley")
abline(model3)
```

Interpretation:

```         
When plotting the data, it can be seen that the data is not normal, following more of an exponential progression. Therefore, the line of best fit generated by the simple linear regression model is not quite fitting and will likely fail by a significant amount more as the dataset (range of xs) get larger. This is further supported by the residuals, of which has a large variance around 0 and of which poorly fits the QQ plot.
```

**End of Assignment 1.2**

What if the assumption 3 is not satisfied?

Assumption 3 says that the error term $\epsilon$ should be independent normal random variable with mean 0 and fixed standard deviation, so we define our model 4 as

$$
y_i=2+4x_i+\epsilon_i^*
$$

where $\epsilon_i^*$ follows a skewed distribution but still with mean 0.

To do that, use

```{r}
eps_2 <- rlnorm(21,0,1)-exp(1/2)
hist(eps_2)
mean(eps_2)
```

## Assignment 1.3

Now mimic the above processes, use the same x1 and eps1, define the second y using y1_4\<-2+4\*x1+eps2, then obtain the scatter plot, check if the relationship between y1_4 and x1 is approximately linear, still run the simple linear regression analysis (store the result in object mod3), but check how reliable the result is, and finally plot the scatter plot together with the regression line.

Remark: When you create the plots, make sure you add the title with your name.

R codes:

```{r}
#Defining x3 and epsilon3
x3 <- seq(from=0,to=2,by=0.1)
eps3 <- rnorm(21,0,1)

#Simulating y1
y1_4 <- 2+4*x3^3+eps_2

#Obtaining the scatter plot of (x1,y1)
plot(y1_4, x3)
title(main="Scatter Plot - Data 4 - JJ McCauley")

#This plot shows a roughly negative linear pattern, so we may use linear regression
model4 <- lm(y1_4~x3)

#Showcasing the results
summary(model4)
anova(model4)
plot(model4)
title(main="Scatter Plot - Model 4 - JJ McCauley")

#Plot the linear regression line on the original plot
plot(y1_4~x3)
title(main="Scatter Plot - Model v Data 4 - JJ McCauley")
abline(model4)
```

Interpretation:

```         
When observing the scatterplot, it is fairly clear that the distribution does not follow a linear progression and therefore a linear regression model is likely not the best model. As expected, when plotting the linear regression line, it is clear that this line will not be a good fit, with the points increasing at a significantly different pattern. This is reinforced by the large residuals which spand a wide range around 0, as well as the poor fit of the residual QQ plot.
```

**End of Assignment 1.3**

**Comment**: In practice, when non-linearity or non-normal pattern are observed, we would not do a simple linear regression analysis on that data set. Instead we would probably try to transform or preprocess the pairs in some way to make them look more like a straight line and then do a simple regression or we might try a non-linear regression.

# Part II: Example with Real-World Data

Read the data file "TestScore.csv". This data set contains MATH 155 midterms, homework, final exam and course average. Call the object of this data file as TestScore

R code for reading the data:

```{r}
TestScore <- read.csv("TestScore.csv")
```

This data is a combined (deidentified) test, homework, final exam and course average data from two sections of Math 155 taught in Fall 2017.

## Assignment 2

1.  Create scatter plots of all variables against course average (test 1 vs. course average, test 2 vs. course average etc. a total of 6 graphs). Which response variable is the strongest in predicting course average in your opinion? How do you decide?

    R codes:

    ```{r}
    #Looping through all of them, displaying their scatterplot
    for(x in colnames(TestScore)) {
      if(x != "CourseAverage") {
        plot(TestScore[[x]], TestScore[["CourseAverage"]], xlab=x, ylab="CourseAverage")
        title(main=paste(x, "vs. Course Average"))
      }
    }
    ```

    Interpretation:

    ```         
    I believe that the test average is the strongest in predicting course average. This is because this distribution depicts a roughly linear line, with the test average very roughly correlating 1:1 with the course average. Additionaly, all data is clumped together around this line, with very few outliers.
    ```

2.  Do a complete regression analysis on all 6 pairs. Report 6 regression line equations and R^2^ values. Which explanatory variable is the best in predicting course average?

    R codes:

    ```{r}
    #Following the same for-loop structure, this taking making models of each and printing the results
    for(x in colnames(TestScore)) {
      if(x != "CourseAverage") {
        curModel <- lm(TestScore[["CourseAverage"]] ~ TestScore[[x]])
        print(x)
        print(summary(curModel))
        #print(anova(curModel))
        plot(TestScore[[x]], TestScore[["CourseAverage"]], xlab=x, ylab="CourseAverage")
        title(main=paste(x, "vs. Course Average"))
        abline(curModel)
      }
    }
    ```

    Interpretation:

    ```         
    Test1: 
      CourseAverage = 6.27546 + (.86694)T1
      R2 = .716

    Test 2:
      CourseAverage = 9.92548 + (.78682)T2
      R2 = .6559

    Test 3: 
      CourseAverage = 26.84768 + (.64584)T3
      R2 = .9008

    Test Average:
      CourseAverage = 2.95034 + (.93800)TA
      R2 = .9548

    Homework:
      CourseAverage = 22.3772 + (.5969)HW
      R2 = .5639

    Final Exam: 
      CourseAverage = 25.15131 + (.74847)FinalExam
      R2 = .74847


    Based on this data, the Test Average (TA) variable is the best at predicting the course average. This is due to the significantly large R2 value of ~.95, suggesting that this model provides the best fit.
    ```

3.  Notice how some final exam scores are zeros. These students didn't show up on the final exam. Do you think those zeros can affect the outcome of regression analysis? Explain.

    ```         
    These zeros are very likely pushing down the regression line for the final exam, with these zeros serving as outliers. In other words, students who do not show up the exam almost indefinitely have above a 0% in the course, which is a misrepresentation. This can be seen on the Final Exam scatterplot, with there being large clumps when x=0, ranging all the way up from 40 to below.
    ```

4.  Now remove pairs of final exam -- course average for those that have zeros in final exam. To do that, you can use the code as

    ```{r}
    TestScore2<-subset(TestScore, FinalExam!=0)
    ```

    Then rerun the regression. Did it change the outcome of the regression analysis? Did it make it stronger or weaker? How do you know?

    R codes:

    ```{r}
    #Rerunning the model loop for the new testScores
    for(x in colnames(TestScore2)) {
      if(x != "CourseAverage") {
        curModel <- lm(TestScore2[["CourseAverage"]] ~ TestScore2[[x]])
        print(x)
        print(summary(curModel))
        #print(anova(curModel))
      }
    }
    ```

    Interpretation:

    ```         
    Test1: 
      CourseAverage = 21.68379 + (.69350)T1
      R2 = .5615

    Test 2:
      CourseAverage = 23.78934 + (.65021)T2
      R2 = .7036

    Test 3: 
      CourseAverage = 31.91105 + (.58085)T3
      R2 = .7922

    Test Average:
      CourseAverage = 10.78352 + (.84219)TA
      R2 = .9143

    Homework:
      CourseAverage = 50.28602 + (.29435)HW
      R2 = .2455

    Final Exam: 
      CourseAverage = 26.504471 + (.72879)FinalExam
      R2 = .7320


    This most representative variable for this analysis remains the same, with Test Average still being the strongest fit variable. However, this outcome did affect the equations and R2 statistics, making the R2 statistic significantly smaller in every case (except for T2, which got larger). This showcases that, with the exception of T2, all variables showcase a weaker fit with this outcome.
    ```

5.  Out of all regression analyses you made, choose the best predictor. Do residuals satisfy necessary conditions? Is the regression analysis valid? Can it be used for estimation and prediction?

    Interpretation:

    ```         
    According to all of the analyses, the Test Average score is the best predictor of Course Average. This is due to it's superior R2 statistic, which reigns over all others as the largest. For this variable, the residuals are reasonable, with the points increasing correspondinly with the regression line in small and random deviations. Additionally, the other assumptions are valid, ensuring that this regression analysis is valid. Additionally, the p-value is significantly smaller than .05 and the R2 value is sufficiently large (.95), meaning that this can be used for estimation and prediction.
    ```

**End of Assignment 2**

# Part III: Special Cases

A statistician named Frank Anscombe made up this data, to illustrate a point about regression.

## Assignment 3

Read the data from data file "FA.csv", store it in an object called "FA"

Run the regression analysis for Y1 on X, Y2 on X, Y3 on X, and Y4 on X4. Compare the four sets of regression output. What do they all have in common? List all of the commonalities.

If you only looked at the regression output, would you think these data sets were alike?

Produce four scatter plots for the same four pairs of columns--put them all on the same graph but in different panels. What was Anscombe trying to illustrate with this example?

R codes:

```{r}
#Reading the CSV
FA <- read.csv("FA.csv")

#Creating each model and printing their summary
model_Y1 <- lm(FA[["Y1"]] ~ FA[["X"]])
print(summary(model_Y1))

model_Y2 <- lm(FA[["Y2"]] ~ FA[["X"]])
print(summary(model_Y2))

model_Y3 <- lm(FA[["Y3"]] ~ FA[["X"]])
print(summary(model_Y3))

model_Y4 <- lm(FA[["Y4"]] ~ FA[["X4"]])
print(summary(model_Y4))

#Creating plots of all sets
par(mfrow=c(2,2)) #setting up a 2x2 plotting layout

plot(FA[["X"]], FA[["Y1"]], xlab="Y1", ylab="X")
    title(main=paste("Y1 vs. X"))
    abline(model_Y1)
    
plot(FA[["X"]], FA[["Y2"]], xlab="Y2", ylab="X")
    title(main=paste("Y2 vs. X"))
    abline(model_Y2)

plot(FA[["X"]], FA[["Y3"]], xlab="Y3", ylab="X")
    title(main=paste("Y3 vs. X"))
    abline(model_Y3)
    
plot(FA[["X4"]], FA[["Y4"]], xlab="Y4", ylab="X4")
    title(main=paste("Y4 vs. X4"))
    abline(model_Y4)
```

Interpretation:

```         
When running the regression analysis on these four sets, they all seem to be nearly identical in their output. They all appear to have nearly identical estimated intercepts coefficient, R2 statistics, p-values, and F-stastics. Similarly, they all have extremely close estimated standard deviations (for both coefficients), Error values, t-values, Pr(>|t|) values, and residual standard errors. The residual quartiles are, however, fairly variant among the four sets.

If looking at just the models, I would believe that these data sets are extremely alike. However, when plotting the scatterplots of the actual data, it becomes apparent that the data is actually very dissumlar. The creator of this data, Anscombe, was likely attempting to illustrate the flaw that simple linear regression will attempt to find a linear line of best fit, even whenever the data is not actually linear. This showcases the importance of checking the conditions for linear regression, as data in violation can produce wildly inaccurate data. Additionally, this showcases the importance for other, potentially more intracate models to be used.
```

**End of Assignment 3**
